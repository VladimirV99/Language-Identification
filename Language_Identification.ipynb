{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d045d2",
   "metadata": {
    "id": "05d045d2"
   },
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80288",
   "metadata": {
    "id": "f4b80288"
   },
   "source": [
    "Language identification of similar languages using an ensemble of recurrent neural networks. Implementation based on the paper [\"LIDE: Language Identification from Text\n",
    "Documents\"](https://arxiv.org/pdf/1701.03682.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62993a6e",
   "metadata": {
    "id": "62993a6e"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b374c6",
   "metadata": {
    "id": "a6b374c6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a29b5",
   "metadata": {},
   "source": [
    "## Make workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they don't exist\n",
    "os.makedirs(os.path.join('datasets/DSLCC-v2.0'), exist_ok=True)\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f536ba6",
   "metadata": {
    "id": "9f536ba6"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd806e0",
   "metadata": {},
   "source": [
    "We use the [DSLCC v2.0](https://github.com/alvations/bayesmax/tree/master/bayesmax/data/DSLCC-v2.0) dataset from the [DSL Shared Task 2015](http://ttg.uni-saarland.de/lt4vardial2015/dsl.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d75c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "844d75c0",
    "outputId": "25c0cb59-a60d-417f-89cd-bc1c0515bee7"
   },
   "outputs": [],
   "source": [
    "# DSLCC v2.0\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/train.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o train.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/train-dev/train.txt\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/devel.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o devel.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/train-dev/devel.txt\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/test.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o test.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/test/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596f314",
   "metadata": {
    "id": "9596f314"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce00216",
   "metadata": {
    "id": "6ce00216"
   },
   "source": [
    "The corpus contains 20,000 instances per language (18,000 training + 2,000 development). Each instance is an excerpt extracted from journalistic texts containing 20 to 100 tokens and tagged with the country of origin of the text. A list of languages and the corresponing codes is shown in the following table:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Group Name</th>\n",
    "        <th>Language Name</th>\n",
    "        <th>Language Code</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>South Eastern Slavic</td>\n",
    "        <td>Bulgarian</td>\n",
    "        <td>bg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Macedonian</td>\n",
    "        <td>mk</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=3>South Western Slavic</td>\n",
    "        <td>Bosnian</td>\n",
    "        <td>bs</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Croatian</td>\n",
    "        <td>hr</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Serbian</td>\n",
    "        <td>sr</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>West-Slavic</td>\n",
    "        <td>Czech</td>\n",
    "        <td>cz</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Slovak</td>\n",
    "        <td>sk</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Ibero-Romance (Spanish)</td>\n",
    "        <td>Peninsular Spanish</td>\n",
    "        <td>es-ES</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Argentinian Spanish</td>\n",
    "        <td>es-AR</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Ibero-Romance (Portugese)</td>\n",
    "        <td>Brazilian Portugese</td>\n",
    "        <td>pt-BR</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>European Portugese</td>\n",
    "        <td>pt-PT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Astronesian</td>\n",
    "        <td>Indonesian</td>\n",
    "        <td>id</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Malay</td>\n",
    "        <td>my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Other</td>\n",
    "        <td>Various Languages</td>\n",
    "        <td>xx</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbf165",
   "metadata": {
    "id": "d1cbf165"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/DSLCC-v2.0/train.txt', sep='\\t', names=['sentence', 'language'])\n",
    "validation = pd.read_csv('datasets/DSLCC-v2.0/devel.txt', sep='\\t', names=['sentence', 'language'])\n",
    "test = pd.read_csv('datasets/DSLCC-v2.0/test.txt', sep='\\t', names=['sentence', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b8b96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a23b8b96",
    "outputId": "47a73370-3727-43e7-94ae-94e16dcb89eb"
   },
   "outputs": [],
   "source": [
    "print(f'Training set size:   {len(train)}')\n",
    "print(f'Validation set size: {len(validation)}')\n",
    "print(f'Test set size:       {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9861f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b9861f8",
    "outputId": "8d70813c-58e6-476d-a380-f2eae66f5cc7"
   },
   "outputs": [],
   "source": [
    "# Print number of instances per label\n",
    "print(train['language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['language'] == 'xx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6b558",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1d6b558",
    "outputId": "bdaa97f7-a120-4dbb-922d-4f6176761ce4"
   },
   "outputs": [],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_UNKNOWN = 'xx'\n",
    "CLASSES = ['bg', 'mk', 'bs', 'hr', 'sr', 'cz', 'sk', 'es-ES', 'es-AR', 'pt-BR', 'pt-PT', 'id', 'my', CLASS_UNKNOWN]\n",
    "CLASS_NAMES = [\n",
    "    'Bulgarian', 'Macedonian', 'Bosnian', 'Croatian', 'Serbian', 'Czech', 'Slovak',\n",
    "    'Peninsular Spanish', 'Argentinian Spanish', 'Brazilian Portuguese', 'European Portuguese',\n",
    "    'Indonesian', 'Malay', 'Other'\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all other language codes to xx\n",
    "def mark_unknown_languages(data):\n",
    "    data['language'].where([x in CLASSES for x in data['language']], CLASS_UNKNOWN, inplace=True)\n",
    "mark_unknown_languages(train)\n",
    "mark_unknown_languages(validation)\n",
    "mark_unknown_languages(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c28d50",
   "metadata": {
    "id": "c0c28d50"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bcbc56",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e126d7",
   "metadata": {
    "id": "72e126d7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2f53c",
   "metadata": {
    "id": "abb2f53c"
   },
   "outputs": [],
   "source": [
    "X_train = train['sentence']\n",
    "y_train = train['language']\n",
    "X_validation = validation['sentence']\n",
    "y_validation = validation['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d7bfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e1d7bfe",
    "outputId": "125cbb5d-b6fd-412d-d381-55e08ccf036e"
   },
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8830035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder for target variable\n",
    "# This is better than get_dummies because here we specify all classes\n",
    "# so all possible classes will have a column and the order will be specified\n",
    "# If the language code is unkown, an error is thrown\n",
    "target_encoder = OneHotEncoder(sparse=False, dtype=np.int32)\n",
    "target_encoder.fit(np.array(CLASSES).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, configure and train a tokenizer \n",
    "def get_tokenizer(data, num_words=None):\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“„”–', num_words=num_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fc468",
   "metadata": {},
   "source": [
    "### Word unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71006ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w1 = get_tokenizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer vocabulary size\n",
    "len(tokenizer_w1.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of occurrences of n-th most common word\n",
    "n = 15_000\n",
    "sorted(tokenizer_w1.word_counts.items(), key=lambda w: w[1], reverse=True)[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words that occurr more than n times\n",
    "n = 50\n",
    "len([x for x in tokenizer_w1.word_counts.items() if x[1] > n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef62822",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNIQUE_WORDS = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2157527",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w1 = get_tokenizer(X_train, NUM_UNIQUE_WORDS)\n",
    "X_train = tokenizer_w1.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max length of train sequences\n",
    "max([len(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42133b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of instances longer than n tokens\n",
    "n = 50\n",
    "len([x for x in X_train if len(x) > n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_TOKENS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_WORD_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72062d71",
   "metadata": {},
   "source": [
    "### Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.util import ngrams\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b71b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAMS_MAX_WORDS = {\n",
    "    2: None,\n",
    "    3: 20000,\n",
    "    4: None,\n",
    "    5: None\n",
    "}\n",
    "\n",
    "def sentence_to_char_ngram(sentence, n):\n",
    "    s = ''.join([c if c not in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“„”–' else ' ' for c in sentence])\n",
    "    tokens = text_to_word_sequence(s)\n",
    "    ngrams_ = [[''.join(ng) for ng in list(ngrams(token, n))] for token in tokens if len(token) >= n]\n",
    "    return ' '.join(chain.from_iterable(ngrams_))\n",
    "\n",
    "def transform_to_char_ngrams(X, n):\n",
    "    X_ngram_train = X.copy()\n",
    "    print(f'{n} - gramming')\n",
    "    return X_ngram_train.progress_apply(lambda sentence: sentence_to_char_ngram(sentence, n))\n",
    "\n",
    "def get_char_ngram_tokenizer(X, n):\n",
    "    tokenizer = get_tokenizer(X, num_words=NGRAMS_MAX_WORDS[n])\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ac682",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, GRU, Dropout, Dense\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84804a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133723f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, recurrent_layer_size, recurrent_dropout_rate=0.0, dropout_rate=0.0, use_lstm=False):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    if use_lstm:\n",
    "        model.add(LSTM(hidden_layer_size, recurrent_dropout=recurrent_dropout_rate, name='lstm'))\n",
    "    else:\n",
    "        model.add(GRU(hidden_layer_size, recurrent_dropout=recurrent_dropout_rate, name='gru'))\n",
    "    model.add(Dropout(rate=dropout_rate, name='dropout'))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax', name='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e386995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model training history\n",
    "def plot_history(history):\n",
    "    plt.plot([-1] + history.epoch, [0.0] + history.history['accuracy'])\n",
    "    plt.plot([-1] + history.epoch, [0.0] + history.history['val_accuracy'])\n",
    "    plt.legend(['Training accuracy', 'Validation accuracy'])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xticks(np.arange(-1, len(history.epoch)), np.arange(len(history.epoch)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced32a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model, compilation and training data, history plot and tokenizer\n",
    "def save_rnn(model, batch_size, history, tokenizer, model_name=None):\n",
    "    recurrent_layer = model.get_layer(index=0)\n",
    "    recurrent_type = recurrent_layer.name\n",
    "    recurrent_units = recurrent_layer.units\n",
    "    recurrent_dropout_rate = recurrent_layer.recurrent_dropout\n",
    "    \n",
    "    dropout_layer = model.get_layer(index=1)\n",
    "    dropout_rate = dropout_layer.rate\n",
    "\n",
    "    epochs = len(history.epoch)\n",
    "    \n",
    "    if not model_name:\n",
    "        model_name = f'model_{epochs}_{batch_size}_{recurrent_type}_{recurrent_units}_{int(100*recurrent_dropout_rate)}_{int(100*dropout_rate)}_{time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    model_path = f'models/{model_name}'\n",
    "    \n",
    "    model.save(model_path)\n",
    "    with open(f'{model_path}/training.txt', 'w') as f:\n",
    "        f.write(f'EPOCHS:            \\t {epochs}\\n')\n",
    "        f.write(f'BATCH SIZE:        \\t {batch_size}\\n')\n",
    "        f.write(f'RECURRENT LAYER:   \\t {recurrent_type}\\n')\n",
    "        f.write(f'RECURRENT UNITS:   \\t {recurrent_units}\\n')\n",
    "        f.write(f'RECURRENT DROPOUT: \\t {recurrent_dropout_rate}\\n')\n",
    "        f.write(f'OUTPUT DROPOUT:    \\t {dropout_rate}\\n')\n",
    "        model.summary(print_fn = lambda x: f.write(x + '\\n'))\n",
    "        f.write(f'ACCURACY:     \\t {history.history[\"accuracy\"]}\\n')\n",
    "        f.write(f'VAL ACCURACY: \\t {history.history[\"val_accuracy\"]}\\n')\n",
    "    plot_history(history)\n",
    "    plt.title(model_name)\n",
    "    plt.savefig(f'{model_path}/history.png')\n",
    "    \n",
    "    with open(f'{model_path}/tokenizer.json', 'w') as f:\n",
    "        f.write(tokenizer.to_json())\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnn(model_name):\n",
    "    with open(f'models/{model_name}/tokenizer.json', 'r') as f:\n",
    "        tokenizer = tokenizer_from_json(f.read())\n",
    "    model = load_model(f'models/{model_name}')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, input_sequences, vocabulary_size, labels, batch_size=32, shuffle=True):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        # TODO move to __getitem__\n",
    "        self.labels = target_encoder.transform(np.asarray(labels).reshape(-1, 1))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # TODO check does this get called automatically anyway\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.input_sequences) / self.batch_size))\n",
    "\n",
    "    # Generate one batch\n",
    "    def __getitem__(self, index):\n",
    "        indexes = np.arange(index*self.batch_size, min((index+1)*self.batch_size, len(self.input_sequences)))\n",
    "        X = to_categorical([self.input_sequences[index] for index in indexes], num_classes=self.vocabulary_size)\n",
    "        # y = target_encoder.transform(np.asarray([labels[index] for index in indexes]).reshape(-1, 1))\n",
    "        y = np.asarray([self.labels[index] for index in indexes])\n",
    "        return X, y\n",
    "\n",
    "    # Update indexes for next epoch\n",
    "    def on_epoch_end(self):\n",
    "        # TODO move to __init__, there is no need to re-arange indexes each epoch\n",
    "        # they will either always or never be shuffled\n",
    "        self.indexes = np.arange(len(self.input_sequences))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGenerator(Sequence):\n",
    "    def __init__(self, input_sequences, vocabulary_size, batch_size=32):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.input_sequences) / self.batch_size))\n",
    "\n",
    "    # Generate one batch\n",
    "    def __getitem__(self, index):\n",
    "        indexes = np.arange(index*self.batch_size, min((index+1)*self.batch_size, len(self.input_sequences)))\n",
    "        X = to_categorical([self.input_sequences[index] for index in indexes], num_classes=self.vocabulary_size)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths = {2: 150, 3: 150}\n",
    "def prepare_n_gram_model(n, Xt, Xv, yt, yv):\n",
    "    print(f'{n}-gramming train set...')\n",
    "    Xt = transform_to_char_ngrams(Xt, n)\n",
    "    print(f'{n}-gramming validation set...')\n",
    "    Xv = transform_to_char_ngrams(Xv, n)\n",
    "    print(f'Tokenizing train set...')\n",
    "    n_tokenizer = get_char_ngram_tokenizer(Xt, n)\n",
    "    print(f'Applying tokenizer to the train set...')\n",
    "    X_ngram_train_tokenized = n_tokenizer.texts_to_sequences(Xt)\n",
    "    print(f'Applying tokenizer to the validation set')\n",
    "    X_ngram_val_tokenized = n_tokenizer.texts_to_sequences(Xv)\n",
    "    print(f'Preprocessing train set...')\n",
    "    X_ngram_train = pad_sequences(X_ngram_train_tokenized, padding='post', maxlen=max_lengths[n])\n",
    "    print(f'Preprocessing validation set...')\n",
    "    X_ngram_val = pad_sequences(X_ngram_val_tokenized, padding='post', maxlen=max_lengths[n])\n",
    "\n",
    "    n_classes = (len(n_tokenizer.word_index.keys()) + 1) if NGRAMS_MAX_WORDS[n] is None else n_tokenizer.num_words\n",
    "\n",
    "    n_model = get_model((X_ngram_train.shape[1], n_classes), recurrent_layer_size=768, dropout_rate=0.35)\n",
    "\n",
    "    train_gen = DataGenerator(X_ngram_train, n_classes, yt, BATCH_SIZE)\n",
    "    val_gen = DataGenerator(X_ngram_val, n_classes, yv, BATCH_SIZE)\n",
    "\n",
    "    return {\"model\": n_model, \"tokenizer\": n_tokenizer, \"X_train\": X_ngram_train, \"X_val\": X_ngram_val, \"y_train\": yt, \"y_val\": yv, \"train_gen\": train_gen, \"val_gen\": val_gen}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd06e20",
   "metadata": {
    "id": "7dd06e20"
   },
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2166496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(devel_X_processor=lambda x: x):\n",
    "    X = devel_X_processor(X_validation.copy())\n",
    "\n",
    "    devel_train_X, devel_test_X, devel_train_Y, devel_test_Y = train_test_split(\n",
    "        X, y_validation, train_size=0.75, stratify=y_validation\n",
    "    )\n",
    "\n",
    "    tokenizer = get_tokenizer(devel_train_X, 10_000)\n",
    "    \n",
    "    devel_train_X = tokenizer.texts_to_sequences(devel_train_X)\n",
    "    devel_test_X = tokenizer.texts_to_sequences(devel_test_X)\n",
    "\n",
    "    devel_train_X = pad_sequences(devel_train_X, padding='post', maxlen=50)\n",
    "    devel_test_X = pad_sequences(devel_test_X, padding='post', maxlen=50)\n",
    "\n",
    "    recurrent_layer_sizes = [768, 1024, 1280]\n",
    "    dropout_rates = [0.2, 0.25, 0.35, 0.4, 0.45]\n",
    "    \n",
    "    results_acc = np.zeros((len(recurrent_layer_sizes), len(dropout_rates)))\n",
    "    results_val_acc = np.zeros((len(recurrent_layer_sizes), len(dropout_rates)))\n",
    "\n",
    "    for i, recurrent_layer_size in enumerate(recurrent_layer_sizes):\n",
    "        for j, dropout_rate in enumerate(dropout_rates):\n",
    "            print('Training network with params:')\n",
    "            print(f' - recurrent_layer_size = {recurrent_layer_size}')\n",
    "            print(f' - dropout_rate      = {dropout_rate}')\n",
    "            \n",
    "            devel_train_generator = DataGenerator(devel_train_X, tokenizer.num_words, devel_train_Y, batch_size=BATCH_SIZE)\n",
    "            devel_test_generator = DataGenerator(devel_test_X, tokenizer.num_words, devel_test_Y, batch_size=BATCH_SIZE)\n",
    "\n",
    "            model = get_model((devel_train_X.shape[1], tokenizer.num_words), recurrent_layer_size, 0.0, dropout_rate)\n",
    "            history = model.fit(\n",
    "                devel_train_generator,\n",
    "                validation_data=devel_test_generator,\n",
    "                epochs=EPOCHS\n",
    "            )\n",
    "            model_name = save_rnn(model, BATCH_SIZE, history, tokenizer)\n",
    "      \n",
    "            results_acc[i][j] = history.history[\"accuracy\"][-1]\n",
    "            results_val_acc[i][j] = history.history[\"val_accuracy\"][-1]\n",
    "            print(f'Results for {recurrent_layer_size}, {dropout_rate} ({i}, {j}):')\n",
    "            print(f'accuracy:     {history.history[\"accuracy\"]}')\n",
    "            print(f'val_accuracy: {history.history[\"val_accuracy\"]}')\n",
    "\n",
    "    grid_search_acc = pd.DataFrame(results_acc, index=recurrent_layer_sizes, columns=dropout_rates)\n",
    "    grid_search_val_acc = pd.DataFrame(results_val_acc, index=recurrent_layer_sizes, columns=dropout_rates)\n",
    "    grid_search_acc.to_csv('grid_search_acc.csv')\n",
    "    grid_search_val_acc.to_csv('grid_search_val_acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec995dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_layer_sizes = [768, 1024, 1280]\n",
    "dropout_rates = [0.2, 0.25, 0.35, 0.4, 0.45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_val_acc = np.asarray([\n",
    "    [0.8206967115402222, 0.8084016442298889, 0.8452953100204468, 0.8305288553237915, 0.8359806537628174],\n",
    "    [0.7321428656578064, 0.8373969793319702, 0.8104395866394043, 0.7388392686843872, 0.838083803653717 ],\n",
    "    [0.8328210115432739, 0.8314549326896667, 0.8317964673042297, 0.7439903616905212, 0.8360655903816223]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf9d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(grid_search_val_acc, cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "plt.yticks(np.arange(len(recurrent_layer_sizes)), recurrent_layer_sizes)\n",
    "plt.xticks(np.arange(len(dropout_rates)), dropout_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7090732",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(grid_search_val_acc)\n",
    "best_recurrent_layer_size = recurrent_layer_sizes[best_index // len(dropout_rates)]\n",
    "best_dropout = dropout_rates[best_index % len(dropout_rates)]\n",
    "print(best_index)\n",
    "print(best_recurrent_layer_size)\n",
    "print(best_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b9fae",
   "metadata": {
    "id": "4b7b9fae"
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model, X_val_model, y_train_model, y_val_model = train_test_split(\n",
    "    X_train, y_train, train_size=0.9, stratify=y_train, random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bdcf2",
   "metadata": {},
   "source": [
    "### Word unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_W1_NAME = 'model_w1_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w1 = get_tokenizer(X_train_model, 10_000)\n",
    "\n",
    "X_train_w1 = tokenizer_w1.texts_to_sequences(X_train_model)\n",
    "X_val_w1 = tokenizer_w1.texts_to_sequences(X_val_model)\n",
    "\n",
    "X_train_w1 = pad_sequences(X_train_w1, padding='post', maxlen=50)\n",
    "X_val_w1 = pad_sequences(X_val_w1, padding='post', maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_w1 = get_model((X_train_w1.shape[1], tokenizer_w1.num_words), best_recurrent_layer_size, 0.0, best_dropout)\n",
    "history_w1 = model_w1.fit(\n",
    "    DataGenerator(X_train_w1, tokenizer_w1.num_words, y_train_model, batch_size=BATCH_SIZE),\n",
    "    validation_data=DataGenerator(X_val_w1, tokenizer_w1.num_words, y_val_model, batch_size=BATCH_SIZE),\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e961db",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rnn(model_w1, BATCH_SIZE, history_w1, tokenizer_w1, model_name=MODEL_W1_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model_w1\n",
    "# model_w1, tokenizer_w1 = load_rnn(MODEL_W1_NAME)\n",
    "# model_w1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_w1 = model_w1.predict(PredictGenerator(X_val_w1, tokenizer_w1.num_words, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135041e0",
   "metadata": {},
   "source": [
    "### Character 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32825cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_C2_NAME = 'c2_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43450f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_model_bundle = prepare_n_gram_model(2, X_train_model, X_val_model, y_train_model, y_val_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_model = c2_model_bundle[\"model\"]\n",
    "c2_train_gen = c2_model_bundle[\"train_gen\"]\n",
    "c2_val_gen = c2_model_bundle[\"val_gen\"]\n",
    "c2_tokenizer = c2_model_bundle[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_history = c2_model.fit(c2_train_gen, validation_data=c2_val_gen, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rnn(c2_model, BATCH_SIZE, c2_history, c2_tokenizer, model_name=MODEL_C2_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_tokenizer = c2_model_bundle['tokenizer']\n",
    "X_val_c2 = c2_model_bundle['X_val']\n",
    "output_c2 = c2_model.predict(PredictGenerator(X_val_c2, c2_tokenizer.num_words, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53171a9b",
   "metadata": {},
   "source": [
    "### Character 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_C3_NAME = 'c3_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d004d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_model_bundle = prepare_n_gram_model(3, X_train_model, X_val_model, y_train_model, y_val_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_model = c3_model_bundle[\"model\"]\n",
    "c3_train_gen = c3_model_bundle[\"train_gen\"]\n",
    "c3_val_gen = c3_model_bundle[\"val_gen\"]\n",
    "c3_tokenizer = c3_model_bundle[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_history = c3_model.fit(c3_train_gen, validation_data=c3_val_gen, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40473886",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rnn(c3_model, BATCH_SIZE, c3_history, c3_tokenizer, model_name=MODEL_C3_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_tokenizer = c3_model_bundle['tokenizer']\n",
    "X_val_c3 = c3_model_bundle['X_val']\n",
    "output_c3 = c3_model.predict(PredictGenerator(X_val_c3, c3_tokenizer.num_words, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386647e",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba754f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_NAME = 'ensemble_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ec228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine outputs\n",
    "# TODO add other models\n",
    "# X_ensemble = pd.DataFrame(np.hstack((output_w1, output_c2, output_c3)))\n",
    "X_ensemble = pd.DataFrame(output_w1)\n",
    "y_ensemble = pd.Series(y_val_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_ensemble))\n",
    "# print(len(y_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ea88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_index = 3\n",
    "# print(X_ensemble.iloc[example_index])\n",
    "# print(y_ensemble.iloc[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_ensemble.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "ensemble = LogisticRegression()\n",
    "scores = cross_val_score(ensemble, X_ensemble, y_ensemble, scoring='accuracy', cv=kf)\n",
    "print(f'All scores: {scores}')\n",
    "print(f'Average score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = LogisticRegression()\n",
    "ensemble.fit(X_ensemble, y_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble model\n",
    "if not os.path.exists(f'models/{ENSEMBLE_NAME}'):\n",
    "    os.mkdir(f'models/{ENSEMBLE_NAME}')\n",
    "with open(f'models/{ENSEMBLE_NAME}/model.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble, f)\n",
    "with open(f'models/{ENSEMBLE_NAME}/weights.txt', 'w') as f:\n",
    "    f.write(f'coef:      {ensemble.coef_}\\n')\n",
    "    f.write(f'intercept: {ensemble.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ensemble model\n",
    "# with open('models/ensemble_final/model.pkl', 'rb') as f:\n",
    "#     ensemble = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble.predict(X_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_ensemble, y_pred, labels=CLASSES)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "disp.plot(ax=ax, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "cm = confusion_matrix(y_ensemble, y_pred, labels=CLASSES)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, index=CLASSES, columns=CLASSES)\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe15710",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Language_Identification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79446e44ba6bee24089cb4961a028f7d7a9fa0f62042f6193d6d5842677d9fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
