{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d045d2",
   "metadata": {
    "id": "05d045d2"
   },
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80288",
   "metadata": {
    "id": "f4b80288"
   },
   "source": [
    "- https://arxiv.org/pdf/1701.03682.pdf\n",
    "- https://cs229.stanford.edu/proj2015/324_report.pdf\n",
    "- https://cs229.stanford.edu/proj2015/324_poster.pdf\n",
    "- https://sites.google.com/view/vardial2021/home\n",
    "- http://ttg.uni-saarland.de/resources/DSLCC/\n",
    "- https://mzampieri.com/publications.html\n",
    "- https://mzampieri.com/papers/dsl2016.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62993a6e",
   "metadata": {
    "id": "62993a6e"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b374c6",
   "metadata": {
    "id": "a6b374c6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a29b5",
   "metadata": {},
   "source": [
    "## Make workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they don't exist\n",
    "os.makedirs(os.path.join('datasets/DSLCC-v2.0'), exist_ok=True)\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f536ba6",
   "metadata": {
    "id": "9f536ba6"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd806e0",
   "metadata": {},
   "source": [
    "We use the [DSLCC v2.0](https://github.com/alvations/bayesmax/tree/master/bayesmax/data/DSLCC-v2.0) dataset from the [DSL Shared Task 2015](http://ttg.uni-saarland.de/lt4vardial2015/dsl.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d75c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "844d75c0",
    "outputId": "25c0cb59-a60d-417f-89cd-bc1c0515bee7"
   },
   "outputs": [],
   "source": [
    "# DSLCC v2.0\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/train.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o train.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/train-dev/train.txt\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/devel.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o devel.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/train-dev/devel.txt\n",
    "if not os.path.exists('datasets/DSLCC-v2.0/test.txt'):\n",
    "    !cd datasets/DSLCC-v2.0 && curl -o test.txt https://raw.githubusercontent.com/alvations/bayesmax/master/bayesmax/data/DSLCC-v2.0/test/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596f314",
   "metadata": {
    "id": "9596f314"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce00216",
   "metadata": {
    "id": "6ce00216"
   },
   "source": [
    "The corpus contains 20,000 instances per language (18,000 training + 2,000 development). Each instance is an excerpt extracted from journalistic texts containing 20 to 100 tokens and tagged with the country of origin of the text. A list of languages and the corresponing codes is shown in the following table:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Group Name</th>\n",
    "        <th>Language Name</th>\n",
    "        <th>Language Code</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>South Eastern Slavic</td>\n",
    "        <td>Bulgarian</td>\n",
    "        <td>bg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Macedonian</td>\n",
    "        <td>mk</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=3>South Western Slavic</td>\n",
    "        <td>Bosnian</td>\n",
    "        <td>bs</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Croatian</td>\n",
    "        <td>hr</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Serbian</td>\n",
    "        <td>sr</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>West-Slavic</td>\n",
    "        <td>Czech</td>\n",
    "        <td>cz</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Slovak</td>\n",
    "        <td>sk</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Ibero-Romance (Spanish)</td>\n",
    "        <td>Peninsular Spanish</td>\n",
    "        <td>es-ES</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Argentinian Spanish</td>\n",
    "        <td>es-AR</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Ibero-Romance (Portugese)</td>\n",
    "        <td>Brazilian Portugese</td>\n",
    "        <td>pt-BR</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>European Portugese</td>\n",
    "        <td>pt-PT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>Astronesian</td>\n",
    "        <td>Indonesian</td>\n",
    "        <td>id</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Malay</td>\n",
    "        <td>my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Other</td>\n",
    "        <td>Various Languages</td>\n",
    "        <td>xx</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbf165",
   "metadata": {
    "id": "d1cbf165"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/DSLCC-v2.0/train.txt', sep='\\t', names=['sentence', 'language'])\n",
    "validation = pd.read_csv('datasets/DSLCC-v2.0/devel.txt', sep='\\t', names=['sentence', 'language'])\n",
    "test = pd.read_csv('datasets/DSLCC-v2.0/test.txt', sep='\\t', names=['sentence', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b8b96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a23b8b96",
    "outputId": "47a73370-3727-43e7-94ae-94e16dcb89eb"
   },
   "outputs": [],
   "source": [
    "print(f'Training set size:   {len(train)}')\n",
    "print(f'Validation set size: {len(validation)}')\n",
    "print(f'Test set size:       {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9861f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b9861f8",
    "outputId": "8d70813c-58e6-476d-a380-f2eae66f5cc7"
   },
   "outputs": [],
   "source": [
    "# Print number of instances per label\n",
    "print(train['language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['language'] == 'xx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6b558",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1d6b558",
    "outputId": "bdaa97f7-a120-4dbb-922d-4f6176761ce4"
   },
   "outputs": [],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use CLASSES with OneHotEncoder and CLASS_NAMES for output\n",
    "CLASS_UNKNOWN = 'xx'\n",
    "CLASSES = ['bg', 'mk', 'bs', 'hr', 'sr', 'cz', 'sk', 'es-ES', 'es-AR', 'pt-BR', 'pt-PT', 'id', 'my', CLASS_UNKNOWN]\n",
    "CLASS_NAMES = [\n",
    "    'Bulgarian', 'Macedonian', 'Bosnian', 'Croatian', 'Serbian', 'Czech', 'Slovak',\n",
    "    'Peninsular Spanish', 'Argentinian Spanish', 'Brazilian Portuguese', 'European Portuguese',\n",
    "    'Indonesian', 'Malay', 'Other'\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119869d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_CLASSES = len(train['language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all other language codes to xx\n",
    "def mark_unknown_languages(data):\n",
    "    data['language'].where([x in CLASSES for x in data['language']], CLASS_UNKNOWN, inplace=True)\n",
    "mark_unknown_languages(train)\n",
    "mark_unknown_languages(validation)\n",
    "mark_unknown_languages(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c28d50",
   "metadata": {
    "id": "c0c28d50"
   },
   "source": [
    "## Common preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e126d7",
   "metadata": {
    "id": "72e126d7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2f53c",
   "metadata": {
    "id": "abb2f53c"
   },
   "outputs": [],
   "source": [
    "X_train = train['sentence']\n",
    "y_train = train['language']\n",
    "X_validation = validation['sentence']\n",
    "y_validation = validation['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d7bfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e1d7bfe",
    "outputId": "125cbb5d-b6fd-412d-d381-55e08ccf036e"
   },
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d2f11",
   "metadata": {
    "id": "0a0d2f11"
   },
   "outputs": [],
   "source": [
    "# y_train = pd.get_dummies(y_train).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2e949",
   "metadata": {
    "id": "b6f2e949",
    "outputId": "d8a7b338-5643-47a4-df7a-667d55f77b93"
   },
   "outputs": [],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8830035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder for target variable\n",
    "# this is better than get_dummies because here we specify all classes\n",
    "# so all possible classes will have a column and the order will be specified\n",
    "# if the language code is unkown, an error is thrown\n",
    "target_encoder = OneHotEncoder(sparse=False, dtype=np.int32)\n",
    "target_encoder.fit(np.array(CLASSES).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder.transform(np.asarray(y_train[30000:30010]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, configure and train a tokenizer \n",
    "def get_tokenizer(data, num_words=None):\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“„”–', num_words=num_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90af97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim and pad data\n",
    "def preprocess_data(X, y, max_length=None):\n",
    "    if max_length is not None:\n",
    "        y = y[[len(x)<=max_length for x in X]]\n",
    "        X = [x for x in X if len(x)<=max_length]\n",
    "    # TODO pre or post padding?\n",
    "    X = pad_sequences(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ac682",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, GRU, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133723f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, hidden_layer_size, dropout_rate): #, recurrent_droupout_rate=0, dropout_rate=0, use_lstm=False):\n",
    "    # TODO Test with LSTM instead of GRU\n",
    "    # TODO Test with dropout after hidden layer\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "        GRU(hidden_layer_size, recurrent_dropout=dropout_rate),\n",
    "        # Dropout(rate=dropout_rate),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e386995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model training history\n",
    "def plot_history(history):\n",
    "    plt.plot(history.epoch, history.history['accuracy'])\n",
    "    plt.plot(history.epoch, history.history['val_accuracy'])\n",
    "    plt.legend(['Training accuracy', 'Validation accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced32a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model, compilation and training data and history plot\n",
    "def save_model(model, epochs, batch_size, hidden_layer_size, dropout_rate):\n",
    "    model_name = f'models/model_{epochs}_{batch_size}_{hidden_layer_size}_{int(100*dropout_rate)}_{time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    model.save(model_name)\n",
    "    with open(f'{model_name}/training.txt', 'w') as f:\n",
    "        f.write(f'EPOCHS:            \\t {EPOCHS}\\n')\n",
    "        f.write(f'BATCH SIZE:        \\t {BATCH_SIZE}\\n')\n",
    "        f.write(f'HIDDEN LAYER SIZE: \\t {HIDDEN_LAYER_SIZE}\\n')\n",
    "        f.write(f'DROPOUT RATE:      \\t {DROPOUT_RATE}\\n')\n",
    "        model.summary(print_fn = lambda x: f.write(x + '\\n'))\n",
    "        f.write(f'ACCURACY:     \\t {history.history[\"accuracy\"]}\\n')\n",
    "        f.write(f'VAL ACCURACY: \\t {history.history[\"val_accuracy\"]}\\n')\n",
    "    plot_history(history)\n",
    "    plt.savefig(f'{model_name}/history.png')\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cdf8e",
   "metadata": {
    "id": "934cdf8e"
   },
   "source": [
    "## Character n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06022d",
   "metadata": {
    "id": "cf06022d"
   },
   "source": [
    "## Word unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74fbdf",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNIQUE_WORDS = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28537c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“„”–', num_words=NUM_UNIQUE_WORDS, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "tokenizer_w1 = get_tokenizer(X_train, NUM_UNIQUE_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6713881",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer_w1.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max word dict index\n",
    "max([max(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea99f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max length of train sequences\n",
    "max([len(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove texts longer than 50 words\n",
    "# Max text length before this step is 2000+ words\n",
    "# y_train = y_train[[len(x)<=50 for x in X_train]]\n",
    "# X_train = [X for X in X_train if len(X)<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_data(X_train, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w1.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_w1.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w1.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in tokenizer_w1.word_counts.items() if x[1] > 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer_w1.word_counts.items(), key=lambda w: w[1], reverse=False)[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b58f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb71462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83655303",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad354a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, input_sequences, vocabulary_size, labels, batch_size=32, shuffle=True):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        # TODO use target_encoder and move to __getitem__\n",
    "#         self.labels = pd.get_dummies(labels).to_numpy()\n",
    "        self.labels = target_encoder.transform(np.asarray(labels).reshape(-1, 1))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.input_sequences) / self.batch_size))\n",
    "\n",
    "    # Generate one batch\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = to_categorical([self.input_sequences[index] for index in indexes], num_classes=self.vocabulary_size)\n",
    "        # y = np.asarray([to_categorical(self.decoder_output[index], num_classes=self.n_classes) for index in indexes])\n",
    "        y = np.asarray([self.labels[index] for index in indexes])\n",
    "        return X, y\n",
    "\n",
    "    # Update indexes for next epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.input_sequences))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c35c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_LAYER_SIZE = 768\n",
    "DROPOUT_RATE = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(X_train, tokenizer_w1.num_words, y_train, batch_size=BATCH_SIZE)\n",
    "# validation_generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model((X_train.shape[1], tokenizer_w1.num_words), HIDDEN_LAYER_SIZE, DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='models/checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "#     validation_data=validation_generator,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, EPOCHS, BATCH_SIZE, HIDDEN_LAYER_SIZE, DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd06e20",
   "metadata": {
    "id": "7dd06e20"
   },
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2166496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de64fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(devel_X_processor=lambda x: x):\n",
    "    X = devel_X_processor(X_validation.copy())\n",
    "\n",
    "    devel_train_X, devel_test_X, devel_train_Y, devel_test_Y = train_test_split(\n",
    "        X, y_validation, train_size=0.75, stratify=y_validation\n",
    "    )\n",
    "\n",
    "    tokenizer = get_tokenizer(devel_train_X, 10_000)\n",
    "    \n",
    "    devel_train_X = tokenizer.texts_to_sequences(devel_train_X)\n",
    "    devel_test_X = tokenizer.texts_to_sequences(devel_test_X)\n",
    "\n",
    "    devel_train_X, devel_train_Y = preprocess_data(devel_train_X, devel_train_Y, 50)\n",
    "    devel_test_X, devel_test_Y = preprocess_data(devel_test_X, devel_test_Y, 50)\n",
    "\n",
    "    hidden_layer_sizes = [768, 1024, 1280]\n",
    "    dropout_rates = [0.2, 0.25, 0.35, 0.4, 0.45]\n",
    "    \n",
    "    results_acc = np.zeros((\n",
    "        len(hidden_layer_sizes), len(dropout_rates)\n",
    "    ))\n",
    "    results_val_acc = np.zeros((\n",
    "        len(hidden_layer_sizes), len(dropout_rates)\n",
    "    ))\n",
    "\n",
    "    for i, hidden_layer_size in enumerate(hidden_layer_sizes):\n",
    "        for j, dropout_rate in enumerate(dropout_rates):\n",
    "#             if i == 0 and j == 0:\n",
    "#                 continue\n",
    "            print('Training network with params:')\n",
    "            print(f' - hidden_layer_size = {hidden_layer_size}')\n",
    "            print(f' - dropout_rate      = {dropout_rate}')\n",
    "            \n",
    "            devel_train_generator = DataGenerator(devel_train_X, tokenizer.num_words, devel_train_Y, batch_size=BATCH_SIZE)\n",
    "            devel_test_generator = DataGenerator(devel_test_X, tokenizer.num_words, devel_test_Y, batch_size=BATCH_SIZE)\n",
    "            # TODO len(tokenizer.word_index)+1 ?\n",
    "            model = get_model((devel_train_X.shape[1], tokenizer.num_words), hidden_layer_size, dropout_rate)\n",
    "            history = model.fit(\n",
    "                devel_train_generator,\n",
    "                validation_data=devel_test_generator,\n",
    "                epochs=EPOCHS\n",
    "            )\n",
    "            model_name = save_model(model)\n",
    "      \n",
    "            results_acc[i][j] = history.history[\"accuracy\"]\n",
    "            results_val_acc[i][j] = history.history[\"val_accuracy\"]\n",
    "            print(f'Results for {hidden_layer_size}, {dropout_rate} ({i}, {j}):')\n",
    "            print(f'accuracy:     {history.history[\"accuracy\"]}')\n",
    "            print(f'val_accuracy: {history.history[\"val_accuracy\"]}')\n",
    "  \n",
    "\n",
    "    grid_search_results_acc_df = pd.DataFrame(results_acc, index=hidden_layer_sizes, columns=dropout_rates)\n",
    "    grid_search_results_val_acc_df = pd.DataFrame(results_val_acc, index=hidden_layer_sizes, columns=dropout_rates)\n",
    "    grid_search_results_acc_df.to_csv('grid_search_acc.csv')\n",
    "    grid_search_results_val_acc_df.to_csv('grid_search_acc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b9fae",
   "metadata": {
    "id": "4b7b9fae"
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Language_Identification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "79446e44ba6bee24089cb4961a028f7d7a9fa0f62042f6193d6d5842677d9fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
